{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR! Session/line number was not unique in database. History logging moved to new session 141\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bmr15315_1.out', 'bmr17160_1.out', 'bmr7339_1.out']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir('data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_df = pd.read_csv('data/bmr15315_1.out', sep = ' ', names=['Residue', 'ShiftCript index'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_tensor = torch.tensor(in_df['ShiftCript index'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "windows = index_tensor.unfold(0, 6, 1)\n",
    "# print(windows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (linear1): Linear(in_features=6, out_features=10, bias=True)\n",
      "  (linear2): Linear(in_features=10, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "#         self.conv1 = nn.Conv1d(6, 36, 2)\n",
    "        self.linear1 = nn.Linear(6, 10)\n",
    "        self.linear2 = nn.Linear(10, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "#         x = F.max_pool1d(F.relu(self.conv1(x)),2)\n",
    "        x = F.relu(self.linear1(x))\n",
    "        print(x)\n",
    "        x = self.linear2(x)\n",
    "        return x\n",
    "    \n",
    "net = Net()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv1): Conv1d(1, 12, kernel_size=(3,), stride=(1,))\n",
      "  (linear1): Linear(in_features=12, out_features=6, bias=True)\n",
      "  (linear2): Linear(in_features=6, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# For some reason 1D convolution and I don't like each other\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv1d(1, 12, 3)  # (in_channels, out_channels, kernel_size)  # DON'T TOUCH FOR NOW\n",
    "        self.linear1 = nn.Linear(12, 6)\n",
    "        self.linear2 = nn.Linear(6, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.max_pool1d(F.relu(self.conv1(x)),4)\n",
    "        print(x)\n",
    "#         print(x.size())\n",
    "        x = x.view(-1, 12)  # Flatten!!! Eureka!!!\n",
    "        print(x)\n",
    "        x = F.relu(self.linear1(x))\n",
    "        x = self.linear2(x)\n",
    "        return x\n",
    "    \n",
    "net = Net()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv1): Conv1d(1, 42, kernel_size=(3,), stride=(1,))\n",
      "  (linear1): Linear(in_features=42, out_features=6, bias=True)\n",
      "  (linear2): Linear(in_features=6, out_features=1, bias=True)\n",
      "  (linear_alpha): Linear(in_features=42, out_features=8, bias=True)\n",
      "  (linear_beta): Linear(in_features=8, out_features=4, bias=True)\n",
      "  (linear_gamma): Linear(in_features=4, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Now with 2 targets, and tuneable hyperparameters\n",
    "number_of_kernels_conv = 42\n",
    "# These 2 not fully understood. These values work. \n",
    "kernel_size = 3\n",
    "pooling_window_size = 3\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv1d(1, int(number_of_kernels_conv), int(kernel_size))  # (in_channels, out_channels, kernel_size)  # DON'T TOUCH FOR NOW\n",
    "        self.linear1 = nn.Linear(int(number_of_kernels_conv * kernel_size / pooling_window_size) , 6)\n",
    "        self.linear2 = nn.Linear(6, 1)\n",
    "        self.linear_alpha = nn.Linear(int(number_of_kernels_conv * kernel_size / pooling_window_size),8)\n",
    "        self.linear_beta = nn.Linear(8,4)\n",
    "        self.linear_gamma = nn.Linear(4,1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.max_pool1d(F.relu(self.conv1(x)), int(pooling_window_size))\n",
    "#         print(x)\n",
    "        print(x.size())\n",
    "        x = x.view(-1, int(number_of_kernels_conv * kernel_size / pooling_window_size))  # Flatten!!! Eureka!!!\n",
    "        print(x)\n",
    "        y = F.relu(self.linear_alpha(x))\n",
    "        y = F.relu(self.linear_beta(y))\n",
    "        y = self.linear_gamma(y)\n",
    "        x = F.relu(self.linear1(x))\n",
    "        x = self.linear2(x)\n",
    "        return x, y\n",
    "    \n",
    "net = Net()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv1): Conv1d(1, 42, kernel_size=(3,), stride=(1,))\n",
      "  (linear1): Linear(in_features=42, out_features=6, bias=True)\n",
      "  (linear2): Linear(in_features=6, out_features=1, bias=True)\n",
      "  (linear_alpha): Linear(in_features=42, out_features=8, bias=True)\n",
      "  (linear_beta): Linear(in_features=8, out_features=4, bias=True)\n",
      "  (linear_gamma): Linear(in_features=4, out_features=1, bias=True)\n",
      ")\n",
      "torch.Size([1, 42, 1])\n",
      "tensor([[0.0000, 0.4292, 0.6018, 0.0000, 0.7921, 0.7279, 0.0000, 0.0000, 0.6149,\n",
      "         0.5699, 0.5943, 0.0192, 0.5870, 0.3810, 0.6495, 0.2749, 0.5849, 0.6829,\n",
      "         0.5876, 0.4669, 0.8300, 0.7069, 0.2250, 0.2040, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.1916, 0.4039, 0.8293, 0.0000, 0.3744, 0.1882,\n",
      "         0.6047, 0.4449, 0.0000, 0.4885, 0.6169, 0.0873]],\n",
      "       grad_fn=<ViewBackward>)\n",
      "(tensor([[-0.3344]], grad_fn=<AddmmBackward>), tensor([[0.4727]], grad_fn=<AddmmBackward>))\n"
     ]
    }
   ],
   "source": [
    "# Now with 2 targets, and tuneable hyperparameters\n",
    "number_of_kernels_conv = 42\n",
    "# These 2 not fully understood. These values work. \n",
    "kernel_size = 3\n",
    "pooling_window_size = 3\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv1d(1, int(number_of_kernels_conv), int(kernel_size))  # (in_channels, out_channels, kernel_size)  # DON'T TOUCH FOR NOW\n",
    "        self.linear1 = nn.Linear(int(number_of_kernels_conv) , 6)\n",
    "        self.linear2 = nn.Linear(6, 1)\n",
    "        self.linear_alpha = nn.Linear(int(number_of_kernels_conv),8)\n",
    "        self.linear_beta = nn.Linear(8,4)\n",
    "        self.linear_gamma = nn.Linear(4,1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.max_pool1d(F.relu(self.conv1(x)), int(pooling_window_size))\n",
    "#         print(x)\n",
    "        print(x.size())\n",
    "        x = x.view(-1, int(number_of_kernels_conv))  # Flatten!!! Eureka!!!\n",
    "        print(x)\n",
    "        y = F.relu(self.linear_alpha(x))\n",
    "        y = F.relu(self.linear_beta(y))\n",
    "        y = self.linear_gamma(y)\n",
    "        x = F.relu(self.linear1(x))\n",
    "        x = self.linear2(x)\n",
    "        return x, y\n",
    "    \n",
    "net = Net()\n",
    "print(net)\n",
    "print(net(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(list(net.named_parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-10.0000,   0.6654,   0.5337,   0.6093,   0.9774,   0.6371])\n"
     ]
    }
   ],
   "source": [
    "# a = windows[0].reshape(-1,1).unsqueeze(0)\n",
    "a = windows[0]\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.0519, -0.5038,  0.4446, -0.8503,  0.4368, -1.3708]]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.randn(1,1,6)  # (Batch size, channels, size of input)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n",
      "torch.Size([42, 1, 3])\n"
     ]
    }
   ],
   "source": [
    "params = list(net.parameters())\n",
    "print(len(params))\n",
    "print(params[0].size())  # conv1's .weight\n",
    "# print(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 42, 1])\n",
      "tensor([[0.2130, 0.1564, 0.0773, 0.0000, 0.0000, 0.2072, 0.4524, 0.0000, 0.5631,\n",
      "         0.9466, 0.5106, 0.0044, 0.2100, 0.1567, 0.7990, 1.0699, 0.7559, 0.3927,\n",
      "         0.2945, 0.0000, 0.2120, 0.0000, 0.1497, 0.4350, 0.1879, 0.0000, 0.0972,\n",
      "         0.5938, 0.2778, 0.7052, 0.0000, 0.0000, 0.7290, 0.0000, 0.0828, 0.3928,\n",
      "         0.3999, 0.1800, 0.7754, 0.3331, 0.0000, 0.0440]],\n",
      "       grad_fn=<ViewBackward>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[0.5315]], grad_fn=<AddmmBackward>),\n",
       " tensor([[0.1521]], grad_fn=<AddmmBackward>))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
